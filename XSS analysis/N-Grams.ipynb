{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# totaldata = pd.read_csv('payload_all.csv',encoding='utf-8')\n",
    "# totaldata.dropna(inplace=True)\n",
    "# totaldata.iloc[6531]\n",
    "\n",
    "train_data = pd.read_csv('train_data.csv',encoding='utf-8')\n",
    "test_data = pd.read_csv('test_data.csv',encoding='utf-8')\n",
    "train_data.columns = ['i', 'payload','type']\n",
    "train_data.drop(['i'], axis=1,inplace=True)\n",
    "train_data.dropna(inplace=True)\n",
    "test_data.columns = ['i', 'payload','type']\n",
    "test_data.drop(['i'], axis=1,inplace=True)\n",
    "test_data.dropna(inplace=True)\n",
    "# print(test_data.isnull().values.any())\n",
    "train_data.loc[train_data['type'] == 'norm', 'class'] = 0\n",
    "train_data.loc[train_data['type'] == 'xss', 'class'] = 1\n",
    "test_data.loc[test_data['type'] == 'norm', 'class'] = 0\n",
    "test_data.loc[test_data['type'] == 'xss', 'class'] = 1\n",
    "\n",
    "\n",
    "# test_data.type.unique()\n",
    "\n",
    "# train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_token_sent(s):\n",
    "#     print('new s incoming')\n",
    "#     print(s)\n",
    "    # Convert to lowercases\n",
    "    try:\n",
    "        s = s.lower()\n",
    "    except:\n",
    "        pass\n",
    "#         print(s)\n",
    "    # Replace all non-alphanumeric characters with spaces\n",
    "    s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "    \n",
    "    \n",
    "    # Break sentence in the token, remove empty tokens\n",
    "    tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "    \n",
    "    #construct new sentence with tokens\n",
    "    sent = \"\"\n",
    "    for i in range(len(tokens)):\n",
    "        if i == 0:\n",
    "            sent = sent + tokens[0]\n",
    "        #elif i != (len(tokens)-1):\n",
    "        #    sent = sent + \" \" + tokens[i]\n",
    "        else:\n",
    "            sent = sent + \" \" + tokens[i]#sent = sent + tokens[i]\n",
    "#     print(sent)\n",
    "    return sent\n",
    "    \n",
    "\n",
    "def generate_token_sent_col(arr):\n",
    "    new_arr = []\n",
    "#     print(arr.size)\n",
    "    for i in range(arr.size):\n",
    "        new_arr.append(generate_token_sent(arr.iloc[i]))\n",
    "    new_arr = np.array(new_arr)\n",
    "    return new_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# totaldata.drop(totaldata.index[[6531,6533]],inplace=True)\n",
    "trainpayload = train_data.payload\n",
    "trainresult = train_data['class']\n",
    "\n",
    "testpayload = test_data.payload\n",
    "testresult = test_data['class']\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(payload, result, test_size=0.33, random_state=42)\n",
    "\n",
    "# print(tot_payload.iloc[6531])\n",
    "\n",
    "trainpayload_new = generate_token_sent_col(trainpayload)\n",
    "testpayload_new = generate_token_sent_col(testpayload)\n",
    "\n",
    "\n",
    "# train_payload = generate_token_sent_col(X_train)\n",
    "# test_payload = generate_token_sent_col(X_test)\n",
    "\n",
    "# print(testpayload_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60295, 82670)\n",
      "(9376, 82670)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "count_vect.fit(trainpayload_new)\n",
    "train_bow = count_vect.transform(trainpayload_new)\n",
    "test_bow = count_vect.transform(testpayload_new)\n",
    "\n",
    "# X_train_counts = count_vect.fit_transform(train_payload)\n",
    "# X_test_counts = count_vect.fit_transform(test_payload)\n",
    "print(train_bow.shape)\n",
    "print(test_bow.shape)\n",
    "# print(X_test_counts.shape)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_total_counts, result, test_size=0.33, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.987094709897611"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(gamma='auto',kernel='linear')\n",
    "clf.fit(train_bow, trainresult)\n",
    "y_pred = clf.predict(test_bow)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(testresult, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.987094709897611"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(testresult, y_pred, average='micro')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60295, 82670)\n",
      "(9376, 82670)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9811220136518771"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(trainpayload_new)\n",
    "Tfidf_trainpayload = vectorizer.transform(trainpayload_new)\n",
    "Tfidf_testpayload = vectorizer.transform(testpayload_new)\n",
    "print(Tfidf_trainpayload.shape)\n",
    "print(Tfidf_testpayload.shape)\n",
    "\n",
    "# TfidfX_train, TfidfX_test, Tfidfy_train, Tfidfy_test = train_test_split(Tfidf_payload, result, test_size=0.33, random_state=42)\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(gamma='auto',kernel='linear')\n",
    "clf.fit(Tfidf_trainpayload, trainresult)\n",
    "tfidf_pred = clf.predict(Tfidf_testpayload)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(testresult, tfidf_pred)\n",
    "f1_score(testresult, tfidf_pred, average='micro')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "def makengrams(sentence,n):\n",
    "    grams = ngrams(sentence.split(), n)\n",
    "    for g in grams:\n",
    "      print g\n",
    "def ngrampayload(payload):\n",
    "    for each in payload:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3342576791808874"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(train_bow, trainresult)\n",
    "y_pred = neigh.predict(test_bow)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(testresult, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
